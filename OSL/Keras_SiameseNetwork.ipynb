{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This part is about training a Siamemese network on a syntethic dataset created from MNIST for Color histogram ####\n",
    "####                                   differentiation                                                             ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importing libraries\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#from tensorflow.keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D            # pooling\n",
    "from tensorflow.keras.layers import Concatenate             # merge\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense  # core\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will use the MNIST Dataset from Keras\n",
    "\n",
    "output_path = r\"C:\\Users\\TRMoussa-PCHP\\OneDrive\\PSC\\psc\\PSC\\notebook\\Deep_Model\\output\"\n",
    "weigths_path = r\"C:\\Users\\TRMoussa-PCHP\\OneDrive\\PSC\\psc\\PSC\\notebook\\Deep_Model\\weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading datas\n",
    "## from mnist\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(xtrain_m, ytrain_m), (xtest_m, ytest_m) = mnist.load_data()\n",
    "xtrain_m = xtrain_m.astype('float32')\n",
    "xtest_m  = xtest_m.astype('float32')\n",
    "xtrain_m /= 255\n",
    "xtest_m  /= 255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Saving training datas\n",
    "\n",
    "with open(os.path.join(output_path,\"train_mnist.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((xtrain_m,ytrain_m),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Saving testing datas\n",
    "\n",
    "with open(os.path.join(output_path,\"test_mnist.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((xtest_m,ytest_m),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Fonctions to initialize bias for trainig\n",
    "\n",
    "def initialize_bias(shape, name=None, dtype = None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Fonctions to initialize weights for trainig\n",
    "\n",
    "def initialize_weights(shape, name=None, dtype = None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for L_2 distance between tensors\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Model with a well known architecture for feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Construction of a siamese network, the bemlow one is popular for CNN tasks\n",
    "###  We modifies the layers inputs to make it usables for our task\n",
    "\n",
    "def get_siamese_model_compatible(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5,5), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (2,2), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (2,2), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TRMoussa-PCHP\\Anaconda3\\envs\\EviPSC\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30, 30, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 30, 30, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 4096)         1382528     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 4096)         0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            4097        lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,386,625\n",
      "Trainable params: 1,386,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model_compatible((30, 30, 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TRMoussa-PCHP\\Anaconda3\\envs\\EviPSC\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datas \n",
    "## from mnist\n",
    "with open(os.path.join(output_path, \"train_mnist.pickle\"), \"rb\") as f:\n",
    "    (xtrain_m, ytrain_m) = pickle.load(f)\n",
    "    \n",
    "    \n",
    "with open(os.path.join(output_path, \"test_mnist.pickle\"), \"rb\") as f:\n",
    "    (xtest_m, ytest_m) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create pairs for mnist train and testing\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n",
    "    for d in range(num_classes):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, num_classes)\n",
    "            dn = (d + inc) % num_classes\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test positive and negative pairs\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "digit_indices = [np.where(ytrain_m == i)[0] for i in range(num_classes)]\n",
    "tr_pairs, tr_y = create_pairs(xtrain_m, digit_indices)\n",
    "\n",
    "digit_indices = [np.where(ytest_m == i)[0] for i in range(num_classes)]\n",
    "te_pairs, te_y = create_pairs(xtest_m, digit_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "17820\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind = 1807\n",
    "#plt.imshow(te_pairs[8950][1], cmap = \"gray\")\n",
    "\n",
    "print(te_y[ind])\n",
    "print(len(te_pairs))\n",
    "\n",
    "image_pair = te_pairs[ind]\n",
    "#plt.imshow(image_pair[0], cmap = plt.cm.gray)\n",
    "plt.subplot(211)\n",
    "plt.imshow(image_pair[0], cmap = plt.cm.gray)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(image_pair[1], cmap = plt.cm.gray)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "#rms = RMSprop()\n",
    "#model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n",
    "#model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
    "#          batch_size=128,\n",
    "#          epochs=epochs,\n",
    "#          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Functions from processing mnist database images in order to produce synthesis histrogram datas for our model\n",
    "\n",
    "def process_digit(digit_image):\n",
    "\n",
    "    digit_image = digit_image.astype(np.float32) / 255\n",
    "\n",
    "    rd = np.random.rand((6))\n",
    "    bool_list = rd > 0.4\n",
    "    \n",
    "    use_GBlur,use_dilate,do_closig,use_MBlur,do_erode_first,do_erode_second = bool_list\n",
    "\n",
    "    target_shape = (30,30)\n",
    "    digit_image = cv2.resize(digit_image, target_shape)\n",
    "\n",
    "    if(use_GBlur):\n",
    "        digit_image = cv2.GaussianBlur(digit_image, (5,5), cv2.BORDER_DEFAULT)\n",
    "\n",
    "    if(use_dilate):\n",
    "        kernel = np.ones((2,2), np.uint8)\n",
    "        digit_image = cv2.dilate(digit_image, kernel, iterations = 1)\n",
    "\n",
    "    if(do_closig):\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        digit_image = cv2.morphologyEx(digit_image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    if(use_MBlur):\n",
    "        digit_image = cv2.medianBlur(digit_image, 5)\n",
    "        \n",
    "    if(do_erode_first):\n",
    "        kernel = np.ones((2,2), np.uint8)\n",
    "        digit_image = cv2.erode(digit_image, kernel)\n",
    "\n",
    "    if(do_erode_second):\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        digit_image = cv2.erode(digit_image, kernel)\n",
    "\n",
    "    return bool_list, digit_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True False  True]\n",
      "(30, 30)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create joined dataset and process inputs to have histograms\n",
    "# for now we will only use MNIST\n",
    "\n",
    "bool_s, image = process_digit(te_pairs[6859][0])\n",
    "plt.imshow(image, cmap = plt.cm.gray)\n",
    "print(bool_s)\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108400 108400\n"
     ]
    }
   ],
   "source": [
    "# (inputs,targets) from digits dataset \n",
    "# (tre_pair,tr_y) from mnist dataset\n",
    "\n",
    "print(len(tr_pairs),len(tr_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Function we will use tho process bothimages of a pair with the same operation in order to preserve similarities\n",
    "\n",
    "def process_digit_pair(digit_image_pair):\n",
    "    \n",
    "    digit_image_pair_res = [0,1]\n",
    "    digit_image_pair_res[0] = digit_image_pair[0].astype(np.float32) / 255\n",
    "    digit_image_pair_res[1] = digit_image_pair[1].astype(np.float32) / 255\n",
    "\n",
    "    \n",
    "    rd = np.random.rand((6))\n",
    "    bool_list = rd > 0.3\n",
    "    \n",
    "    use_GBlur,use_dilate,do_closig,use_MBlur,do_erode_first,do_erode_second = bool_list\n",
    "\n",
    "    target_shape = (30,30)\n",
    "    for i in range(2):\n",
    "        digit_image = cv2.resize(digit_image_pair[i], target_shape)\n",
    "\n",
    "        if(use_GBlur):\n",
    "            digit_image = cv2.GaussianBlur(digit_image, (5,5), cv2.BORDER_DEFAULT)\n",
    "\n",
    "        if(use_dilate):\n",
    "            kernel = np.ones((2,2), np.uint8)\n",
    "            digit_image = cv2.dilate(digit_image, kernel, iterations = 1)\n",
    "\n",
    "        if(do_closig):\n",
    "            kernel = np.ones((3,3), np.uint8)\n",
    "            digit_image = cv2.morphologyEx(digit_image, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        if(use_MBlur):\n",
    "            digit_image = cv2.medianBlur(digit_image, 5)\n",
    "\n",
    "        if(do_erode_first):\n",
    "            kernel = np.ones((3,3), np.uint8)\n",
    "            digit_image = cv2.erode(digit_image, kernel)\n",
    "\n",
    "        if(do_erode_second):\n",
    "            kernel = np.ones((3,3), np.uint8)\n",
    "            digit_image = cv2.erode(digit_image, kernel)\n",
    "            \n",
    "        digit_image_pair_res[i] = digit_image\n",
    "\n",
    "    return bool_list, digit_image_pair_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False  True  True  True]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ind = 7095\n",
    "bool_s, image_pair = process_digit_pair(tr_pairs[ind])\n",
    "#plt.imshow(image_pair[0], cmap = plt.cm.gray)\n",
    "plt.subplot(211)\n",
    "plt.imshow(image_pair[0], cmap = plt.cm.gray)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(image_pair[1], cmap = plt.cm.gray)\n",
    "\n",
    "plt.show()\n",
    "print(bool_s)\n",
    "print(tr_y[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adapt datas from Mnist , Synthetisation for histogram data\n",
    "#tr_pairs, tr_y\n",
    "tr_pairs_proceded = []\n",
    "for ind in range(len(tr_pairs)):\n",
    "    bool_s, image_pair = process_digit_pair(tr_pairs[ind])\n",
    "    tr_pairs_proceded.append(image_pair)\n",
    "\n",
    "tr_y_proceded = tr_y\n",
    "    \n",
    "#te_pairs, te_y\n",
    "te_pairs_proceded = []\n",
    "for ind in range(len(te_pairs)):\n",
    "    bool_s, image_pair = process_digit_pair(te_pairs[ind])\n",
    "    te_pairs_proceded.append(image_pair)\n",
    "    \n",
    "te_y_proceded = te_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "ind = 1927\n",
    "\n",
    "image_pair = tr_pairs_proceded[ind]\n",
    "plt.subplot(211)\n",
    "plt.imshow(image_pair[0], cmap = plt.cm.gray)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(image_pair[1], cmap = plt.cm.gray)\n",
    "\n",
    "plt.show()\n",
    "print(tr_y_proceded[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pairs_proceded = np.array(tr_pairs_proceded)\n",
    "te_pairs_proceded = np.array(te_pairs_proceded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108400, 2, 30, 30)"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_pairs_proceded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() > 0.5\n",
    "    return np.mean(pred == y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114181 samples, validate on 18891 samples\n",
      "Epoch 1/10\n",
      "114181/114181 [==============================] - 692s 6ms/sample - loss: 0.3480 - val_loss: 0.3359\n",
      "Epoch 2/10\n",
      "114181/114181 [==============================] - 728s 6ms/sample - loss: 0.3294 - val_loss: 0.3283\n",
      "Epoch 3/10\n",
      "114181/114181 [==============================] - 701s 6ms/sample - loss: 0.3218 - val_loss: 0.3236\n",
      "Epoch 4/10\n",
      "114181/114181 [==============================] - 633s 6ms/sample - loss: 0.3173 - val_loss: 0.3217\n",
      "Epoch 5/10\n",
      "114181/114181 [==============================] - 634s 6ms/sample - loss: 0.3126 - val_loss: 0.3178\n",
      "Epoch 6/10\n",
      "114181/114181 [==============================] - 601s 5ms/sample - loss: 0.3092 - val_loss: 0.3130\n",
      "Epoch 7/10\n",
      "114181/114181 [==============================] - 585s 5ms/sample - loss: 0.3057 - val_loss: 0.3132\n",
      "Epoch 8/10\n",
      "114181/114181 [==============================] - 582s 5ms/sample - loss: 0.3020 - val_loss: 0.3112\n",
      "Epoch 9/10\n",
      "114181/114181 [==============================] - 583s 5ms/sample - loss: 0.2994 - val_loss: 0.3064\n",
      "Epoch 10/10\n",
      "114181/114181 [==============================] - 619s 5ms/sample - loss: 0.2969 - val_loss: 0.3081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f3065830f0>"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training step\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "model.fit([tr_pairs_proceded[:, 0].reshape(108400,30,30,1), tr_pairs_proceded[:, 1].reshape(108400,30,30,1)], tr_y_proceded,\n",
    "          batch_size=128,\n",
    "          epochs=epochs,\n",
    "          validation_data=([te_pairs_proceded[:, 0].reshape(17820,30,30,1), te_pairs_proceded[:, 1].reshape(17820,30,30,1)], te_y_proceded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving the model\n",
    "\n",
    "#from tensorflow.keras.models import load_model\n",
    "model.save(os.path.join(weigths_path, 'model_siamese_first.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TRMoussa-PCHP\\Anaconda3\\envs\\EviPSC\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\TRMoussa-PCHP\\Anaconda3\\envs\\EviPSC\\lib\\site-packages\\tensorflow_core\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "### Loading the model\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "#help(tf.keras.initialisations)\n",
    "with CustomObjectScope({'initialize_weights':initialize_weights,'initialize_bias':initialize_bias}):\n",
    "    loadded_model = load_model(os.path.join(weigths_path, 'model_siamese_first.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 30, 30, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 30, 30, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 4096)         1382528     input_11[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 4096)         0           sequential_5[1][0]               \n",
      "                                                                 sequential_5[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            4097        lambda_2[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,386,625\n",
      "Trainable params: 1,386,625\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loadded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy on training set: 93.49%\n",
      "* Accuracy on test set: 93.23%\n"
     ]
    }
   ],
   "source": [
    "# Compute final accuracy on training and test sets\n",
    "\n",
    "y_pred = model.predict([tr_pairs_proceded[:, 0].reshape(108400,30,30,1), tr_pairs_proceded[:, 1].reshape(108400,30,30,1)])\n",
    "tr_acc = compute_accuracy(tr_y_proceded, y_pred)\n",
    "y_pred = model.predict([te_pairs_proceded[:, 0].reshape(17820,30,30,1), te_pairs_proceded[:, 1].reshape(17820,30,30,1)])\n",
    "te_acc = compute_accuracy(te_y_proceded, y_pred)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIQAAAD7CAYAAACrMDyzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOiUlEQVR4nO2dXWwU1RvGn5fSKvIhtGCttCgYbCASv9Cg+EGCVUQTL0yjaIwXJF4oiSbGKHrllVwYvfIGI8ELrZL4RTSGEEP9SNQUtdqWph8YkIWGtqIUCgqF87/Y4ex557/bDruzMzu7zy/Z9JkzZzvH+nDec86ceUeMMSDkAtPibgApLWgIoqAhiIKGIAoagihoCKIoyBAisk5E+kRkUEReDqtRJD4k33UIEakC0A+gBUAKQAeADcaYfeE1j0TN9AK+exuAQWPMHwAgIh8CeBhATkOICFfBSodRY8wCf2EhIWMhgEPOccorI8ngYLbCQnoIyVL2fz2AiDwN4OkCrkMipBBDpAA0OceNAI74KxljtgLYCjBkJIFCQkYHgKUislhEagA8BmBnOM0icZF3D2GMmRCRTQB2AagCsM0Y0xNay0gs5D3tzOtiDBmlxM/GmJX+Qq5UEgUNQRQ0BFHQEERBQxAFDUEUNARR0BBEQUMQRSE3t8hFMH165k89c+ZMq+fNm6fqjY+PW338+PGitefMmTNZy9lDEAUNQRQ0BFFwDFEkqqur1fGCBZnti9ddd53VK1fqG44dHR1Wd3d3W33ixImwm5gV9hBEQUMQBUNGiIhk9h3PnTtXnVuxYoXVDz74oNV33HGHqudOBw8cOGD1sWPHrC7mpib2EERBQxAFQ0aBuGFi9uzZVjc3N6t6Dz30kNWbNm2y+q233lL1Ojs7rXZXKqPa+8oegihoCKKgIYiCY4gCcVckFy1aZPW9996r6j3zzDNWv/HGG1Z/+eWXql5fX5/VJ0+eDK2dQZmyhxCRbSIyLCLdTlmtiOwWkQHv57zJfgdJDkFCxnYA63xlLwP42hizFMDX3jEpA6YMGcaYb0XkGl/xwwDWePo9AO0AXgqxXSXLtGn631Btba3V7o2qxx9/XNV77bXXrHbDxMDAgKrnhonz588X1tg8yHdQWW+MGQIA7+cV4TWJxEnRB5VMGJIs8jXEURFpMMYMiUgDgOFcFcshYYgbJi6//HJ17sYbb7T60UcftbqtrU3V27kzkzqjv7/f6lOnToXWzjDIN2TsBPCUp58C8Hk4zSFxE2Ta2QbgBwDNIpISkY0AtgBoEZEBpNMSbiluM0lUBJllbMhxam3IbSElAFcqA3DppZda7e6HBPRml3/++cfqr776StUbHBy0+vTp02E3MTR4L4MoaAiiYMjIQVVVldWNjY1Wr1mzRtV75JFHrH7xxRetdm9SAfHcqMoH9hBEQUMQBUOGg7s/8rLLLrPa3ULf2tqqvvPZZ59Z3dXVZXWprUAGhT0EUdAQRMGQ4eDOLBYuzLz6w51ZNDW5LwAAXn/9dasPHsy8giJXQo5Shz0EUdAQREFDEEVFjyH8+yPdBGC33nqr1U888YTVn376qfpOT0/mFSHuVDPK106ECXsIoqAhiKKiQ4abOxIA6uvrrV69erXV7v6F7777Tn1neDiznXRiYiLsJkYOewiioCGIouJChnsDy00xDADLli2z2t0a99FHH1ntJvQAdLrApM4sXNhDEAUNQRQ0BFFU3BjCvaNZV1enzrmrk729vVbv3r3baveOJlAeU02XIE9uNYnIHhHpFZEeEXnOK2fSkDIkSMiYAPCCMWYZgFUAnhWR5WDSkLIkyKN8QwAu5II4ISK9ABYioUlDampqrPZvdrnnnnusfuedd6x2p5pjY2PqO+Uw1XS5qEGll0nmJgA/gUlDypLAg0oRmQXgYwDPG2PG3AWeKb7HhCEJIpAhRKQaaTO8b4z5xCsOlDSkFBKGuOadM2eO1e72egAYHR21+scff7R6ZGTE6nPnzhWjiSVDkFmGAHgXQK8x5k3nFJOGlCFBeojVAJ4E0CUiF0ZXryCdJGSHl0DkTwCtOb5PEkSQWcb3AHINGJg0pMyoiJXKGTNmWL1kyRKr167Vfv7ll1+sTqVSVpfbauRk8F4GUdAQRFGWIcO/RpJrqnnDDTeoejt27LDafQd3JcEegihoCKIoy5Dhx00r6KYmbm9vV/UOHz5sdbndtAoKewiioCGIoiJChpsS0M0D5T51BejFqDheXlIKsIcgChqCKGgIopAop1dxbZBxp51XXJHZ6efurwT0mKLcHtHLws/GmJX+QvYQREFDEAUNQRQ0BFHQEERBQxAFDUEUNARR0BBEQUMQBQ1BFDQEUUR9c2sEwDiA0anqFpn5bAOuNsYs8BdGaggAEJG92e6ysQ2lAUMGUdAQRBGHIbbGcE0/bEMOIh9DkNKGIYMoIjWEiKwTkT4RGRSRSBKdisg2ERkWkW6nLNIsvEnKBhyZIUSkCsDbAB4AsBzABi8jbrHZDmCdryzqLLzJyQZsjInkA+B2ALuc480ANkd07WsAdDvHfQAaPN0AoC+qv4N3zc8BtMTdjmyfKEPGQgCHnOOUVxYHsWXhLfVswFEaIlsmu4qa4vizAcfdnmxEaYgUADfbeCOAIxFe3+Wol30Xk2XhDZPJsgFH2Y6piNIQHQCWishiEakB8BjS2XDjINIsvInKBhzxYGo9gH4A+wG8GtE125B+vcNZpHupjQDqkB7VD3g/a4vchjuRDo+/A+j0PuujbkeQD1cqiYIrlURBQxBFQYaIYymaFJe8xxDeUnQ/0ituKaRnERuMMfvCax6JmkKSjt0GYNAY8wcAiMiHSL+YLach4koYQrIyarLsqSwkZJTSUjS5eA5mKyykhwi0FM2XsCWLQgwRaCnalMBL2EhwCgkZpbQUTUIi7x7CGDMhIpsA7AJQBWCbMaYntJaRWKiItIQkK0xLSKaGhiAKGoIoaAiioCGIgoYgChqCKGgIoqAhiKIiXsIWlGnTMv8+qqqqrD579mzRrlldXa2OZ86cafV///1n9b///mt1MVeX2UMQBQ1BFDQEUVT0GMIdMwDArFmzrHZf3Pb3338XrQ2zZ89Wx01NmT1HIyMjWXUxxzTsIYiChiCKig4Z/infggWZXelu+Dh16lTR2jB//nx13NzcbPXx48etjmojE3sIoqAhiKLiQoa7Ajlnzhx1bvHixVnrpVKporXnqquuUsfuK6bdUHXu3LmitcGFPQRR0BBEQUMQRUWMIdI5v9LMmDHD6oaGBlVv6dKlVo+OZl52419NDJNrr71WHe/fv99qd6pZMtPOUsgVTaIjSMjYjvhzRZOImDJkGGO+9dLxujwMYI2n3wPQDuClENsVKjU1NVbX1dVZ7a4KTsaiRYtCb9MFGhsb1bEbMuIg30FlyeVoJuFQ9EElE4Yki3wNcVREGowxQ1PlaI4jYcj06fo/a+7cuVYvWbLE6pUr9cPPBw4cyPr76uvrw2scgHnzgo3B40gqm2/IKL0czSQUgkw72wD8AKBZRFIishHAFgAtIjKAdFrCLcVtJomKILOMDTlOrQ25LaQEKJuVSnd/5GT7FG+55Rar/auEucYQYXPzzTdbfejQIXVuYmLC6iSNIUiZQkMQRaJDRq6bVv5p4ooVK6y+6667rJ5sO7t/r2Oh1NbWWn3llVda/dtvv6l67qaYM2fOhNqGILCHIAoagigSHTIuueQSq90wcf3116t6d999t9X333+/1V988UURW6dZtWqV1T09mfyu/v2ax44ds5ohg8QODUEUiUpt7IYIQIcJdyZx3333qXrLl2d/57z7ZBRQ3K3ug4ODVu/du9fq3t5eVe/w4cNWnzx50uoi/H9iamMyNTQEUdAQRJGoaae7Ggnop7Xdc52dnaqe/zgI7qaa1tZWq92t+n727Nlj9TfffKPO7duXeTede0NreFjvLXJXKnlzi8QODUEUiQoZbq5GQE/R/FPIQmlpacla7r/p9cEHH1i9c2fmlWP+MOWGhvHxcav9N9jiCBMu7CGIgoYgikSFDDfVL6AfyHVvCrn7JCbDfaILCLbd3j97aG9vt/rXX3+12r81zm37+fPnA7UvDthDEAUNQRQ0BFEkagzhn5K5W9aD4m7X948hgtDR0aGO3RXIo0ePWu2fIsc9nQxKkCe3mkRkj4j0ikiPiDznlTNpSBkSJGRMAHjBGLMMwCoAz4rIcjBpSFkS5FG+IQAXckGcEJFeAAuRsKQhheBuVOnq6lLn3Oll3DemwuCiBpVeJpmbAPwEJg0pSwIPKkVkFoCPATxvjBkLuvjDhCHJIpAhRKQaaTO8b4z5xCsOlDQkjoQhYeM+XdXf36/OuSuk+cx6So0gswwB8C6AXmPMm84pJg0pQ4L0EKsBPAmgS0Qu3NN9BekkITu8BCJ/AmjN8X2SIILMMr4HkGvAwKQhZUaiVirjwl2NdDflAMDp06etTupU04X3MoiChiAKhowADAwMWO1/h2cpb3bJB/YQREFDEEXFhYzJZgJu7id3v+aRI0esdmcVU/2+JMIegihoCKKo6JDhTxDihomxsTGr//rrr5zfKTfYQxAFDUEUNARRVNwYwsW/VX5oaMhqd0eY+7R2ucMegihoCKKgIYiChiAKGoIoaAiioCGIgoYgChqCKGgIoqAhiIKGIIqo36gzAmAcwOhUdYvMfLYBVxtjFvgLIzUEAIjI3myv9mEbSgOGDKKgIYgiDkNsjeGaftiGHEQ+hiClDUMGUURqCBFZJyJ9IjIoIpEkOhWRbSIyLCLdTlmkWXiTlA04MkOISBWAtwE8AGA5gA1eRtxisx3AOl9Z1Fl4k5MN2BgTyQfA7QB2OcebAWyO6NrXAOh2jvsANHi6AUBfVH8H75qfA2iJux3ZPlGGjIUA3NfMpLyyOIgtC2+pZwOO0hDZMtlV1BTHnw047vZkI0pDpAA0OceNAI7kqFtsjnrZdzFZFt4wmSwbcJTtmIooDdEBYKmILBaRGgCPIZ0NNw4izcKbqGzAEQ+m1gPoB7AfwKsRXbMN6dc7nEW6l9oIoA7pUf2A97O2yG24E+nw+DuATu+zPup2BPlwpZIouFJJFDQEUdAQREFDEAUNQRQ0BFHQEERBQxDF/wB27O6h1lRl2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Prediction with siamese network 0.0037182376254349947\n",
      "Correlation distance = 0.38779598378359476\n"
     ]
    }
   ],
   "source": [
    "### Observation of the correctness of the model\n",
    "\n",
    "ind = 11001\n",
    "image_pair = tr_pairs_proceded[ind]\n",
    "plt.subplot(211)\n",
    "plt.imshow(image_pair[0], cmap = plt.cm.gray)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(image_pair[1], cmap = plt.cm.gray)\n",
    "\n",
    "plt.show()\n",
    "print(tr_y_proceded[ind])\n",
    "\n",
    "#print(tr_pairs_proceded[ind, 0].reshape(1,30,30,1).shape)\n",
    "prediction = loadded_model.predict([tr_pairs_proceded[ind, 0].reshape(1,30,30,1), tr_pairs_proceded[ind, 1].reshape(1,30,30,1)])\n",
    "\n",
    "distance = dist = cv2.compareHist(tr_pairs_proceded[ind, 0], tr_pairs_proceded[ind, 1], cv2.HISTCMP_CORREL)\n",
    "\n",
    "print(\"Prediction with siamese network {}\".format(prediction[0,0]))\n",
    "\n",
    "print(\"Correlation distance = {}\".format(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second base_model using a minimal architecture for improving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We define a new create_pair method for data labelling\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n",
    "    for d in range(num_classes):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, num_classes)\n",
    "            dn = (d + inc) % num_classes\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "        # We reuse the non used rest\n",
    "        for i in range(n+1, len(digit_indices[d])-1):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [0]\n",
    "            \n",
    "    return np.array(pairs), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training+test positive and negative pairs\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "digit_indices = [np.where(ytrain_m == i)[0] for i in range(num_classes)]\n",
    "tr_pairs, tr_y = create_pairs(xtrain_m, digit_indices)\n",
    "\n",
    "digit_indices = [np.where(ytest_m == i)[0] for i in range(num_classes)]\n",
    "te_pairs, te_y = create_pairs(xtest_m, digit_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adapt datas from Mnist , Synthetisation for histogram data\n",
    "#tr_pairs, tr_y\n",
    "tr_pairs_proceded = []\n",
    "for ind in range(len(tr_pairs)):\n",
    "    bool_s, image_pair = process_digit_pair(tr_pairs[ind])\n",
    "    tr_pairs_proceded.append(image_pair)\n",
    "\n",
    "tr_y_proceded = tr_y\n",
    "    \n",
    "#te_pairs, te_y\n",
    "te_pairs_proceded = []\n",
    "for ind in range(len(te_pairs)):\n",
    "    bool_s, image_pair = process_digit_pair(te_pairs[ind])\n",
    "    te_pairs_proceded.append(image_pair)\n",
    "    \n",
    "te_y_proceded = te_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "ind = 7096\n",
    "\n",
    "image_pair = tr_pairs_proceded[ind]\n",
    "plt.subplot(211)\n",
    "plt.imshow(image_pair[0], cmap = plt.cm.gray)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(image_pair[1], cmap = plt.cm.gray)\n",
    "\n",
    "plt.show()\n",
    "print(tr_y_proceded[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pairs_proceded = np.array(tr_pairs_proceded)\n",
    "te_pairs_proceded = np.array(te_pairs_proceded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114181, 2, 30, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_pairs_proceded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TRMoussa-PCHP\\Anaconda3\\envs\\EviPSC\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\TRMoussa-PCHP\\Anaconda3\\envs\\EviPSC\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 114181 samples, validate on 18891 samples\n",
      "Epoch 1/50\n",
      "114181/114181 [==============================] - 22s 191us/sample - loss: 0.0500 - accuracy: 0.9590 - val_loss: 0.0219 - val_accuracy: 0.9681\n",
      "Epoch 2/50\n",
      "114181/114181 [==============================] - 9s 79us/sample - loss: 0.0286 - accuracy: 0.9819 - val_loss: 0.0170 - val_accuracy: 0.9744\n",
      "Epoch 3/50\n",
      "114181/114181 [==============================] - 9s 78us/sample - loss: 0.0245 - accuracy: 0.9844 - val_loss: 0.0155 - val_accuracy: 0.9765\n",
      "Epoch 4/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0221 - accuracy: 0.9854 - val_loss: 0.0153 - val_accuracy: 0.9771\n",
      "Epoch 5/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0207 - accuracy: 0.9864 - val_loss: 0.0148 - val_accuracy: 0.9784\n",
      "Epoch 6/50\n",
      "114181/114181 [==============================] - 9s 76us/sample - loss: 0.0198 - accuracy: 0.9870 - val_loss: 0.0149 - val_accuracy: 0.9785\n",
      "Epoch 7/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0189 - accuracy: 0.9876 - val_loss: 0.0140 - val_accuracy: 0.9789\n",
      "Epoch 8/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0184 - accuracy: 0.9871 - val_loss: 0.0138 - val_accuracy: 0.9799\n",
      "Epoch 9/50\n",
      "114181/114181 [==============================] - 9s 76us/sample - loss: 0.0180 - accuracy: 0.9871 - val_loss: 0.0129 - val_accuracy: 0.9815\n",
      "Epoch 10/50\n",
      "114181/114181 [==============================] - 9s 76us/sample - loss: 0.0176 - accuracy: 0.9871 - val_loss: 0.0144 - val_accuracy: 0.9788\n",
      "Epoch 11/50\n",
      "114181/114181 [==============================] - 9s 76us/sample - loss: 0.0168 - accuracy: 0.9878 - val_loss: 0.0137 - val_accuracy: 0.9811\n",
      "Epoch 12/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0166 - accuracy: 0.9880 - val_loss: 0.0133 - val_accuracy: 0.9818\n",
      "Epoch 13/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0163 - accuracy: 0.9882 - val_loss: 0.0138 - val_accuracy: 0.9803\n",
      "Epoch 14/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0156 - accuracy: 0.9891 - val_loss: 0.0118 - val_accuracy: 0.9837\n",
      "Epoch 15/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0162 - accuracy: 0.9882 - val_loss: 0.0127 - val_accuracy: 0.9821\n",
      "Epoch 16/50\n",
      "114181/114181 [==============================] - 9s 78us/sample - loss: 0.0161 - accuracy: 0.9886 - val_loss: 0.0131 - val_accuracy: 0.9813\n",
      "Epoch 17/50\n",
      "114181/114181 [==============================] - 9s 79us/sample - loss: 0.0156 - accuracy: 0.9889 - val_loss: 0.0133 - val_accuracy: 0.9819\n",
      "Epoch 18/50\n",
      "114181/114181 [==============================] - 9s 78us/sample - loss: 0.0153 - accuracy: 0.9891 - val_loss: 0.0139 - val_accuracy: 0.9803\n",
      "Epoch 19/50\n",
      "114181/114181 [==============================] - 9s 78us/sample - loss: 0.0155 - accuracy: 0.9887 - val_loss: 0.0126 - val_accuracy: 0.9819\n",
      "Epoch 20/50\n",
      "114181/114181 [==============================] - 9s 78us/sample - loss: 0.0156 - accuracy: 0.9886 - val_loss: 0.0131 - val_accuracy: 0.9819\n",
      "Epoch 21/50\n",
      "114181/114181 [==============================] - 9s 81us/sample - loss: 0.0153 - accuracy: 0.9886 - val_loss: 0.0118 - val_accuracy: 0.9828\n",
      "Epoch 22/50\n",
      "114181/114181 [==============================] - 9s 80us/sample - loss: 0.0152 - accuracy: 0.9889 - val_loss: 0.0125 - val_accuracy: 0.9825\n",
      "Epoch 23/50\n",
      "114181/114181 [==============================] - 9s 83us/sample - loss: 0.0152 - accuracy: 0.9889 - val_loss: 0.0120 - val_accuracy: 0.9824\n",
      "Epoch 24/50\n",
      "114181/114181 [==============================] - 9s 81us/sample - loss: 0.0150 - accuracy: 0.9885 - val_loss: 0.0118 - val_accuracy: 0.9830\n",
      "Epoch 25/50\n",
      "114181/114181 [==============================] - 9s 79us/sample - loss: 0.0142 - accuracy: 0.9892 - val_loss: 0.0119 - val_accuracy: 0.9842\n",
      "Epoch 26/50\n",
      "114181/114181 [==============================] - 9s 79us/sample - loss: 0.0144 - accuracy: 0.9895 - val_loss: 0.0127 - val_accuracy: 0.9825\n",
      "Epoch 27/50\n",
      "114181/114181 [==============================] - 9s 79us/sample - loss: 0.0147 - accuracy: 0.9888 - val_loss: 0.0118 - val_accuracy: 0.9840\n",
      "Epoch 28/50\n",
      "114181/114181 [==============================] - 9s 79us/sample - loss: 0.0145 - accuracy: 0.9891 - val_loss: 0.0118 - val_accuracy: 0.9841\n",
      "Epoch 29/50\n",
      "114181/114181 [==============================] - 9s 81us/sample - loss: 0.0144 - accuracy: 0.9892 - val_loss: 0.0131 - val_accuracy: 0.9814\n",
      "Epoch 30/50\n",
      "114181/114181 [==============================] - 9s 80us/sample - loss: 0.0142 - accuracy: 0.9895 - val_loss: 0.0108 - val_accuracy: 0.9861\n",
      "Epoch 31/50\n",
      "114181/114181 [==============================] - 9s 79us/sample - loss: 0.0140 - accuracy: 0.9897 - val_loss: 0.0118 - val_accuracy: 0.9851\n",
      "Epoch 32/50\n",
      "114181/114181 [==============================] - 9s 80us/sample - loss: 0.0139 - accuracy: 0.9895 - val_loss: 0.0122 - val_accuracy: 0.9836\n",
      "Epoch 33/50\n",
      "114181/114181 [==============================] - 8s 74us/sample - loss: 0.0140 - accuracy: 0.9893 - val_loss: 0.0121 - val_accuracy: 0.9835\n",
      "Epoch 34/50\n",
      "114181/114181 [==============================] - 9s 75us/sample - loss: 0.0142 - accuracy: 0.9892 - val_loss: 0.0122 - val_accuracy: 0.9834\n",
      "Epoch 35/50\n",
      "114181/114181 [==============================] - 9s 75us/sample - loss: 0.0139 - accuracy: 0.9892 - val_loss: 0.0123 - val_accuracy: 0.9845\n",
      "Epoch 36/50\n",
      "114181/114181 [==============================] - 9s 75us/sample - loss: 0.0144 - accuracy: 0.9889 - val_loss: 0.0128 - val_accuracy: 0.9824\n",
      "Epoch 37/50\n",
      "114181/114181 [==============================] - 9s 76us/sample - loss: 0.0145 - accuracy: 0.9884 - val_loss: 0.0118 - val_accuracy: 0.9850\n",
      "Epoch 38/50\n",
      "114181/114181 [==============================] - 9s 76us/sample - loss: 0.0143 - accuracy: 0.9886 - val_loss: 0.0115 - val_accuracy: 0.9850\n",
      "Epoch 39/50\n",
      "114181/114181 [==============================] - 10s 86us/sample - loss: 0.0138 - accuracy: 0.9893 - val_loss: 0.0117 - val_accuracy: 0.9848\n",
      "Epoch 40/50\n",
      "114181/114181 [==============================] - 10s 92us/sample - loss: 0.0138 - accuracy: 0.9891 - val_loss: 0.0119 - val_accuracy: 0.9841\n",
      "Epoch 41/50\n",
      "114181/114181 [==============================] - 9s 78us/sample - loss: 0.0137 - accuracy: 0.9893 - val_loss: 0.0120 - val_accuracy: 0.9843\n",
      "Epoch 42/50\n",
      "114181/114181 [==============================] - 9s 77us/sample - loss: 0.0140 - accuracy: 0.9886 - val_loss: 0.0116 - val_accuracy: 0.9851\n",
      "Epoch 43/50\n",
      "114181/114181 [==============================] - 9s 78us/sample - loss: 0.0136 - accuracy: 0.9889 - val_loss: 0.0125 - val_accuracy: 0.9847\n",
      "Epoch 44/50\n",
      "114181/114181 [==============================] - 9s 80us/sample - loss: 0.0139 - accuracy: 0.9889 - val_loss: 0.0126 - val_accuracy: 0.9840\n",
      "Epoch 45/50\n",
      "114181/114181 [==============================] - 8s 74us/sample - loss: 0.0135 - accuracy: 0.9890 - val_loss: 0.0115 - val_accuracy: 0.9841\n",
      "Epoch 46/50\n",
      "114181/114181 [==============================] - 8s 74us/sample - loss: 0.0136 - accuracy: 0.9890 - val_loss: 0.0117 - val_accuracy: 0.9853\n",
      "Epoch 47/50\n",
      "114181/114181 [==============================] - 9s 75us/sample - loss: 0.0132 - accuracy: 0.9897 - val_loss: 0.0121 - val_accuracy: 0.9850\n",
      "Epoch 48/50\n",
      "114181/114181 [==============================] - 9s 80us/sample - loss: 0.0140 - accuracy: 0.9884 - val_loss: 0.0115 - val_accuracy: 0.9854\n",
      "Epoch 49/50\n",
      "114181/114181 [==============================] - 9s 78us/sample - loss: 0.0138 - accuracy: 0.9888 - val_loss: 0.0115 - val_accuracy: 0.9846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      "114181/114181 [==============================] - 8s 73us/sample - loss: 0.0136 - accuracy: 0.9891 - val_loss: 0.0128 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22b27a33f28>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "#from keras.datasets import mnist\n",
    "#from keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n",
    "\n",
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "\n",
    "input_shape = (30,30,1)\n",
    "# network definition\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape=input_shape)\n",
    "input_b = Input(shape=input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "#def euclidean_distance(vects):\n",
    "#    x, y = vects\n",
    "#    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "#    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "\n",
    "# Add a customized layer to compute the absolute difference between the encodings\n",
    "L2_layer = Lambda(lambda tensors:K.sqrt(K.maximum(K.sum(K.square(tensors[0] - tensors[1]), axis=1, keepdims=True), K.epsilon())))\n",
    "distance = L2_layer([processed_a, processed_b])\n",
    "\n",
    "\n",
    "#distance = Lambda(euclidean_distance,\n",
    "#                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "model_m = Model([input_a, input_b], distance)\n",
    "\n",
    "# train\n",
    "rms = RMSprop()\n",
    "model_m.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n",
    "model_m.fit([tr_pairs_proceded[:, 0].reshape(114181,30,30,1), tr_pairs_proceded[:, 1].reshape(114181,30,30,1)], tr_y_proceded,\n",
    "          batch_size=128,\n",
    "          epochs=epochs,\n",
    "          validation_data=([te_pairs_proceded[:, 0].reshape(18891,30,30,1), te_pairs_proceded[:, 1].reshape(18891,30,30,1)], te_y_proceded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy on training set: 99.17%\n",
      "* Accuracy on test set: 98.50%\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "\n",
    "# compute final accuracy on training and test sets\n",
    "y_pred = model_m.predict([tr_pairs_proceded[:, 0].reshape(114181,30,30,1), tr_pairs_proceded[:, 1].reshape(114181,30,30,1)])\n",
    "tr_acc = compute_accuracy(tr_y_proceded, y_pred)\n",
    "y_pred = model_m.predict([te_pairs_proceded[:, 0].reshape(18891,30,30,1), te_pairs_proceded[:, 1].reshape(18891,30,30,1)])\n",
    "te_acc = compute_accuracy(te_y_proceded, y_pred)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Prediction with siamese network 0.9996837722428609\n",
      "Correlation distance = 1.0\n"
     ]
    }
   ],
   "source": [
    "ind = 114  #11111\n",
    "image_pair = tr_pairs_proceded[ind]\n",
    "plt.subplot(211)\n",
    "plt.imshow(image_pair[0], cmap = plt.cm.gray)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.imshow(image_pair[1], cmap = plt.cm.gray)\n",
    "\n",
    "plt.show()\n",
    "print(tr_y_proceded[ind])\n",
    "\n",
    "#print(tr_pairs_proceded[ind, 0].reshape(1,30,30,1).shape)\n",
    "prediction = model_m.predict([tr_pairs_proceded[ind, 0].reshape(1,30,30,1), tr_pairs_proceded[ind, 1].reshape(1,30,30,1)])\n",
    "\n",
    "distance = dist = cv2.compareHist(tr_pairs_proceded[ind, 0], tr_pairs_proceded[ind, 1], cv2.HISTCMP_CORREL)\n",
    "\n",
    "print(\"Prediction with siamese network {}\".format(1 - prediction[0,0]))\n",
    "\n",
    "print(\"Correlation distance = {}\".format(distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Network\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model_m.save(os.path.join(weigths_path, 'model_siamese.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import CustomObjectScope\n",
    "#help(tf.keras.initialisations)\n",
    "with CustomObjectScope({'initialize_weights':initialize_weights,'initialize_bias':initialize_bias,\n",
    "                       'contrastive_loss': contrastive_loss}):\n",
    "    loadded_model = load_model(os.path.join(weigths_path, 'model_siamese.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy on training set: 99.17%\n",
      "* Accuracy on test set: 98.50%\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "\n",
    "# compute final accuracy on training and test sets\n",
    "y_pred = loadded_model.predict([tr_pairs_proceded[:, 0].reshape(114181,30,30,1), tr_pairs_proceded[:, 1].reshape(114181,30,30,1)])\n",
    "tr_acc = compute_accuracy(tr_y_proceded, y_pred)\n",
    "y_pred = loadded_model.predict([te_pairs_proceded[:, 0].reshape(18891,30,30,1), te_pairs_proceded[:, 1].reshape(18891,30,30,1)])\n",
    "te_acc = compute_accuracy(te_y_proceded, y_pred)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
